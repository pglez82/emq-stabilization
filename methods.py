import quapy as qp
from quapy.method.aggregative import AggregativeSoftQuantifier  
from sklearn.base import BaseEstimator
from quapy.data import LabelledCollection
import numpy as np
from abstention.calibration import NoBiasVectorScaling, TempScaling, VectorScaling



class EMQ(AggregativeSoftQuantifier):
    """
    `Expectation Maximization for Quantification <https://ieeexplore.ieee.org/abstract/document/6789744>`_ (EMQ),
    aka `Saerens-Latinne-Decaestecker` (SLD) algorithm.
    EMQ consists of using the well-known `Expectation Maximization algorithm` to iteratively update the posterior
    probabilities generated by a probabilistic classifier and the class prevalence estimates obtained via
    maximum-likelihood estimation, in a mutually recursive way, until convergence.

    This implementation also gives access to the heuristics proposed by `Alexandari et al. paper
    <http://proceedings.mlr.press/v119/alexandari20a.html>`_. These heuristics consist of using, as the training
    prevalence, an estimate of it obtained via k-fold cross validation (instead of the true training prevalence),
    and to recalibrate the posterior probabilities of the classifier.

    :param classifier: a sklearn's Estimator that generates a classifier
    :param val_split: specifies the data used for generating classifier predictions. This specification
        can be made as float in (0, 1) indicating the proportion of stratified held-out validation set to
        be extracted from the training set; or as an integer, indicating that the predictions
        are to be generated in a `k`-fold cross-validation manner (with this integer indicating the value
        for `k`, default 5); or as a collection defining the specific set of data to use for validation.
        Alternatively, this set can be specified at fit time by indicating the exact set of data
        on which the predictions are to be generated. This hyperparameter is only meant to be used when the
        heuristics are to be applied, i.e., if a recalibration is required. The default value is None (meaning
        the recalibration is not required). In case this hyperparameter is set to a value other than None, but
        the recalibration is not required (recalib=None), a warning message will be raised.
    :param exact_train_prev: set to True (default) for using the true training prevalence as the initial observation;
        set to False for computing the training prevalence as an estimate of it, i.e., as the expected
        value of the posterior probabilities of the training instances.
    :param recalib: a string indicating the method of recalibration.
        Available choices include "nbvs" (No-Bias Vector Scaling), "bcts" (Bias-Corrected Temperature Scaling,
        default), "ts" (Temperature Scaling), and "vs" (Vector Scaling). Default is None (no recalibration).
    :param n_jobs: number of parallel workers. Only used for recalibrating the classifier if `val_split` is set to
        an integer `k` --the number of folds.
    """

    MAX_ITER = 1000
    EPSILON = 1e-4

    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None):
        self.classifier = qp._get_classifier(classifier)
        self.val_split = val_split
        self.exact_train_prev = exact_train_prev
        self.recalib = recalib
        self.n_jobs = n_jobs
        self.callback = callback
        self.log_trajectory = False

    @classmethod
    def EMQ_BCTS(cls, classifier: BaseEstimator, n_jobs=None):
        """
        Constructs an instance of EMQ using the best configuration found in the `Alexandari et al. paper
        <http://proceedings.mlr.press/v119/alexandari20a.html>`_, i.e., one that relies on Bias-Corrected Temperature
        Scaling (BCTS) as a recalibration function, and that uses an estimate of the training prevalence instead of
        the true training prevalence.

        :param classifier: a sklearn's Estimator that generates a classifier
        :param n_jobs: number of parallel workers.
        :return: An instance of EMQ with BCTS
        """
        return EMQ(classifier, val_split=5, exact_train_prev=False, recalib='bcts', n_jobs=n_jobs)

    def _check_init_parameters(self):
        if self.val_split is not None:
            if self.exact_train_prev and self.recalib is None:
                raise RuntimeWarning(f'The parameter {self.val_split=} was specified for EMQ, while the parameters '
                      f'{self.exact_train_prev=} and {self.recalib=}. This has no effect and causes an unnecessary '
                      f'overload.')
        else:
            if self.recalib is not None:
                print(f'[warning] The parameter {self.recalib=} requires the val_split be different from None. '
                      f'This parameter will be set to 5. To avoid this warning, set this value to a float value '
                      f'indicating the proportion of training data to be used as validation, or to an integer '
                      f'indicating the number of folds for kFCV.')
                self.val_split=5

    def classify(self, instances):
        """
        Provides the posterior probabilities for the given instances. If the classifier was required
        to be recalibrated, then these posteriors are recalibrated accordingly.

        :param instances: array-like of shape `(n_instances, n_dimensions,)`
        :return: np.ndarray of shape `(n_instances, n_classes,)` with posterior probabilities
        """
        posteriors = self.classifier.predict_proba(instances)
        if hasattr(self, 'calibration_function') and self.calibration_function is not None and self.calib_ok:
            try:
                cal_posteriors = self.calibration_function(posteriors)
                posteriors = cal_posteriors
            except:
                pass
        return posteriors

    def aggregation_fit(self, classif_predictions: LabelledCollection, data: LabelledCollection):
        """
        Trains the aggregation function of EMQ. This comes down to recalibrating the posterior probabilities
        ir requested.

        :param classif_predictions: a :class:`quapy.data.base.LabelledCollection` containing,
            as instances, the posterior probabilities issued by the classifier and, as labels, the true labels
        :param data: a :class:`quapy.data.base.LabelledCollection` consisting of the training data
        """
        if self.recalib is not None:
            P, y = classif_predictions.Xy
            if self.recalib == 'nbvs':
                calibrator = NoBiasVectorScaling()
            elif self.recalib == 'bcts':
                calibrator = TempScaling(bias_positions='all')
            elif self.recalib == 'ts':
                calibrator = TempScaling()
            elif self.recalib == 'vs':
                calibrator = VectorScaling()
            else:
                raise ValueError('invalid param argument for recalibration method; available ones are '
                                 '"nbvs", "bcts", "ts", and "vs".')

            if not np.issubdtype(y.dtype, np.number):
                y = np.searchsorted(data.classes_, y)
            self.calib_ok = True
            try:
                self.calibration_function = calibrator(P, np.eye(data.n_classes)[y], posterior_supplied=True)
            except:
                self.calib_ok = False

        if self.exact_train_prev:
            self.train_prevalence = data.prevalence()
        else:
            train_posteriors = classif_predictions.X
            if self.recalib is not None and self.calib_ok:
                train_posteriors = self.calibration_function(train_posteriors)
            self.train_prevalence = F.prevalence_from_probabilities(train_posteriors)

    def aggregate(self, classif_posteriors, epsilon=EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors

    def predict_proba(self, instances, epsilon=EPSILON):
        """
        Returns the posterior probabilities updated by the EM algorithm.

        :param instances: np.ndarray of shape `(n_instances, n_dimensions)`
        :param epsilon: error tolerance
        :return: np.ndarray of shape `(n_instances, n_classes)`
        """
        classif_posteriors = self.classify(instances)
        priors, posteriors, _ = self.EM(self.train_prevalence, classif_posteriors, epsilon)
        return posteriors

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EPSILON):
        """
        Computes the `Expectation Maximization` routine.

        :param tr_prev: array-like, the training prevalence
        :param posterior_probabilities: `np.ndarray` of shape `(n_instances, n_classes,)` with the
            posterior probabilities
        :param epsilon: float, the threshold different between two consecutive iterations
            to reach before stopping the loop
        :return: a tuple with the estimated prevalence values (shape `(n_classes,)`) and
            the corrected posterior probabilities (shape `(n_instances, n_classes,)`)
        """
        Px = posterior_probabilities
        Ptr = np.copy(tr_prev)

        if np.prod(Ptr) == 0:  # some entry is 0; we should smooth the values to avoid 0 division
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)  # qs (the running estimate) is initialized as the training prevalence

        trajectory = [qs.copy()]

        s, converged = 0, False
        qs_prev_ = None
        while not converged and s < EMQ.MAX_ITER:
            # E-step: ps is Ps(y|xi)
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step:
            qs = ps.mean(axis=0)
            trajectory.append(qs.copy())

            if qs_prev_ is not None and qp.error.mae(qs, qs_prev_) < epsilon and s > 10:
                converged = True

            qs_prev_ = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory


class EMQPosteriorSmoothing(EMQ):
    """
    EMQ variant with posterior smoothing applied before the E-step.
    Adds a small epsilon to all posterior probabilities to prevent instability
    from exact zeros or ones in the posteriors.
    
    :param epsilon_smoothing: small value to add to each posterior for smoothing
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, epsilon_smoothing=1e-5):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        self.epsilon_smoothing = epsilon_smoothing

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, epsilon_smoothing=1e-5):
        """
        Computes EM with posterior smoothing applied to input probabilities before the E-step.
        """
        # Posterior smoothing: avoid exact zeros or ones
        Px = posterior_probabilities + epsilon_smoothing
        Px /= Px.sum(axis=1, keepdims=True)  # re-normalize to ensure valid probability distributions

        Ptr = np.copy(tr_prev)
        if np.prod(Ptr) == 0:  # smooth prior if needed
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]
        s, converged = 0, False
        qs_prev_ = None

        while not converged and s < cls.MAX_ITER:
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)
            qs = ps.mean(axis=0)
            trajectory.append(qs.copy())

            if qs_prev_ is not None and qp.error.mae(qs, qs_prev_) < epsilon and s > 10:
                converged = True

            qs_prev_ = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory
    
    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon,self.epsilon_smoothing)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors

class EMQMomentum(EMQ):
    """
    EMQ variant with momentum-based M-step.
    A momentum term is applied to smooth and accelerate the updates of the prevalence vector Q(Y).

    :param momentum: momentum factor beta ∈ [0, 1). Higher values retain more memory of previous steps.
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, momentum=0.9):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert 0 <= momentum < 1, "momentum must be in [0, 1)"
        self.momentum = momentum

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, momentum=0.9):
        """
        EM routine with momentum-based updates in the M-step.
        """
        Px = posterior_probabilities
        Ptr = np.copy(tr_prev)
        if np.prod(Ptr) == 0:
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]

        s = 0
        converged = False
        qs_prev = np.copy(qs)
        delta_prev = np.zeros_like(qs)

        while not converged and s < cls.MAX_ITER:
            # E-step
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step with momentum
            qs_new = ps.mean(axis=0)
            delta = momentum * delta_prev + (1 - momentum) * (qs_new - qs)
            qs = qs + delta

            trajectory.append(qs.copy())

            if qp.error.mae(qs, qs_prev) < epsilon and s > 10:
                converged = True

            delta_prev = delta
            qs_prev = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory
    
    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon,self.momentum)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors

class EMQEntropyReg(EMQ):
    """
    EMQ variant with entropy regularization in the M-step.
    Instead of using the mean of posteriors, it maximizes a regularized objective:
        L(Q) = <Q_hat, log Q> + eta * H(Q)

    :param eta: regularization strength (eta > 0 favors smoother distributions)
    :param lr: learning rate for projected gradient ascent
    :param max_inner_iter: max iterations for inner optimization
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, eta=0.01, lr=0.1, max_inner_iter=100):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        self.eta = eta
        self.lr = lr
        self.max_inner_iter = max_inner_iter

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON,
           eta=0.01, lr=0.1, max_inner_iter=100):
        """
        EM algorithm with entropy-regularized M-step.
        """
        Px = posterior_probabilities
        Ptr = np.copy(tr_prev)
        if np.prod(Ptr) == 0:
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]
        s = 0
        converged = False
        qs_prev = None

        while not converged and s < cls.MAX_ITER:
            # --- E-step ---
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            Q_hat = ps.mean(axis=0)  # standard M-step estimate

            # --- M-step: optimize entropy-regularized objective ---
            qs = cls._optimize_with_entropy(Q_hat, eta=eta, lr=lr, max_iter=max_inner_iter)

            trajectory.append(qs.copy())

            if qs_prev is not None and qp.error.mae(qs, qs_prev) < epsilon and s > 10:
                converged = True

            qs_prev = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory

    @staticmethod
    def _optimize_with_entropy(Q_hat, eta, lr, max_iter):
        """
        Projected gradient ascent to solve:
            max_Q sum_c Q_hat[c] * log(Q[c]) + eta * H(Q)
        subject to Q in simplex.
        """
        K = len(Q_hat)
        Q = np.full(K, 1.0 / K)

        for _ in range(max_iter):
            grad = Q_hat / (Q + 1e-12) + eta * (-1 - np.log(Q + 1e-12))
            Q += lr * grad
            Q = np.clip(Q, 1e-10, None)  # avoid zeros
            Q /= Q.sum()  # project back to simplex

        return Q

    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon,self.eta,self.lr,self.max_inner_iter)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors


class EMQDirichletMAP(EMQ):
    """
    EMQ variant that uses a symmetric Dirichlet prior in the M-step (MAP estimation).
    This prevents zero estimates and adds smoothing to the class prevalence vector.

    :param alpha: Dirichlet prior parameter (α > 1 adds smoothing)
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, alpha=1.1):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert alpha > 0, "alpha must be > 0"
        self.alpha = alpha

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, alpha=1.1):
        """
        EM with MAP estimation using a symmetric Dirichlet prior in the M-step.
        """
        Px = posterior_probabilities
        Ptr = np.copy(tr_prev)
        if np.prod(Ptr) == 0:
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]
        s, converged = 0, False
        qs_prev = None
        N = Px.shape[0]
        K = Px.shape[1]

        while not converged and s < cls.MAX_ITER:
            # E-step
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step with Dirichlet prior (MAP estimation)
            qs = (ps.sum(axis=0) + alpha - 1) / (N + K * (alpha - 1))
            trajectory.append(qs.copy())

            if qs_prev is not None and qp.error.mae(qs, qs_prev) < epsilon and s > 10:
                converged = True

            qs_prev = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory
    
    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon,self.alpha)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors


class EMQTempScaling(EMQ):
    """
    EMQ variant that applies temperature scaling to the posterior probabilities before the E-step.

    :param tau: temperature parameter τ > 0. Values τ < 1 sharpen the distribution, τ > 1 smooth it.
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, tau=1.0):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert tau > 0, "temperature τ must be > 0"
        self.tau = tau

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, tau=1.0):
        """
        EM with temperature scaling applied to posterior probabilities before the E-step.
        """
        # Apply temperature scaling to the posterior probabilities
        Px = np.power(posterior_probabilities, 1.0 / tau)
        Px /= Px.sum(axis=1, keepdims=True)

        Ptr = np.copy(tr_prev)
        if np.prod(Ptr) == 0:
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]
        s, converged = 0, False
        qs_prev_ = None

        while not converged and s < cls.MAX_ITER:
            # E-step
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step
            qs = ps.mean(axis=0)
            trajectory.append(qs.copy())

            if qs_prev_ is not None and qp.error.mae(qs, qs_prev_) < epsilon and s > 10:
                converged = True

            qs_prev_ = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory

    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon,self.tau)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors


class EMQDamping(EMQ):
    """
    Smooth EM with Prior Momentum.
    This variant modifies the M-step to apply exponential smoothing (momentum)
    to the estimated prevalence vector.

    :param momentum: smoothing factor in [0,1]. Higher means slower updates.
    """
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None, damping=0.5, callback=None):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert 0.0 <= damping <= 1.0, "damping must be in [0, 1]"
        self.damping = damping
        

    @classmethod
    def EM(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, damping=0.5):
        Px = posterior_probabilities
        Ptr = np.copy(tr_prev)

        if np.prod(Ptr) == 0:  # avoid zero division
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)  # initialize with training prevalence
        s, converged = 0, False
        qs_prev_ = None

        trajectory = [qs.copy()]

        while not converged and s < EMQ.MAX_ITER:
            # E-step:
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step with damping:
            qs_new = ps.mean(axis=0)

            delta = 0.05  # max allowed change in any class
            qs_new = np.clip(qs_new, qs - delta, qs + delta)
            qs_new = np.maximum(qs_new, 1e-12)  # avoid zeros
            qs_new = qs_new / qs_new.sum()
            

            qs = damping * qs + (1 - damping) * qs_new

            if qs_prev_ is not None and qp.error.mae(qs, qs_prev_) < epsilon and s > 10:
                converged = True

            trajectory.append(qs.copy())
            qs_prev_ = qs
            s += 1

        if not converged:
            print('[warning] EMQDamping: max iterations reached, might not have converged')

        return qs, ps, trajectory

    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        # Pass the instance momentum to the EM classmethod
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon, self.damping)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors

    def predict_proba(self, instances, epsilon=EMQ.EPSILON):
        classif_posteriors = self.classify(instances)
        return self.EM(self.train_prevalence, classif_posteriors, epsilon, self.damping)[1]
    
class EMQConfidentSubset(EMQ):
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True, recalib=None,
                 n_jobs=None, callback=None, tau=1.0):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert 0 < tau <= 1, "tau must be in (0, 1]"
        self.tau = tau

    def aggregate(self, classif_posteriors, epsilon=EMQ.EPSILON):
        priors, posteriors, trajectory = self.EM_subset_m(
            self.train_prevalence,
            classif_posteriors,
            epsilon,
            tau=self.tau
        )
        if self.log_trajectory and self.callback is not None:
            self.callback(trajectory)
        return priors

    @classmethod
    def EM_subset_m(cls, tr_prev, posterior_probabilities, epsilon=EMQ.EPSILON, tau=1.0):
        Px = posterior_probabilities.astype(np.float64)
        Ptr = tr_prev.astype(np.float64)
        N, K = Px.shape

        if np.prod(Ptr) == 0:
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)
        trajectory = [qs.copy()]
        s, converged = 0, False
        qs_prev = None

        while not converged and s < cls.MAX_ITER:
            # E-step: recompute posteriors under current prevalence estimate
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            # M-step: use subset of top tau confidence
            if tau < 1.0:
                max_probs = Px.max(axis=1)
                threshold = np.quantile(max_probs, 1 - tau)
                subset_mask = max_probs >= threshold
                ps_subset = ps[subset_mask]
            else:
                ps_subset = ps

            qs = ps_subset.mean(axis=0)
            trajectory.append(qs.copy())

            if qs_prev is not None and qp.error.mae(qs, qs_prev) < epsilon and s > 10:
                converged = True

            qs_prev = qs
            s += 1

        if not converged:
            print('[warning] EMQConfidentSubset: max iterations reached; convergence not guaranteed.')

        return qs, ps, trajectory