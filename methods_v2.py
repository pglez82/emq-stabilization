import quapy as qp
from quapy.method.aggregative import AggregativeSoftQuantifier  
from sklearn.base import BaseEstimator
from quapy.data import LabelledCollection
import numpy as np
from abstention.calibration import NoBiasVectorScaling, TempScaling, VectorScaling
import inspect
from functools import partial


def PosteriorSmoothing(posterior_probabilities, epsilon_smoothing):
    Px = posterior_probabilities + epsilon_smoothing
    Px /= Px.sum(axis=1, keepdims=True)

class EMQ(AggregativeSoftQuantifier):
    """
    `Expectation Maximization for Quantification <https://ieeexplore.ieee.org/abstract/document/6789744>`_ (EMQ),
    aka `Saerens-Latinne-Decaestecker` (SLD) algorithm.
    EMQ consists of using the well-known `Expectation Maximization algorithm` to iteratively update the posterior
    probabilities generated by a probabilistic classifier and the class prevalence estimates obtained via
    maximum-likelihood estimation, in a mutually recursive way, until convergence.

    This implementation also gives access to the heuristics proposed by `Alexandari et al. paper
    <http://proceedings.mlr.press/v119/alexandari20a.html>`_. These heuristics consist of using, as the training
    prevalence, an estimate of it obtained via k-fold cross validation (instead of the true training prevalence),
    and to recalibrate the posterior probabilities of the classifier.

    :param classifier: a sklearn's Estimator that generates a classifier
    :param val_split: specifies the data used for generating classifier predictions. This specification
        can be made as float in (0, 1) indicating the proportion of stratified held-out validation set to
        be extracted from the training set; or as an integer, indicating that the predictions
        are to be generated in a `k`-fold cross-validation manner (with this integer indicating the value
        for `k`, default 5); or as a collection defining the specific set of data to use for validation.
        Alternatively, this set can be specified at fit time by indicating the exact set of data
        on which the predictions are to be generated. This hyperparameter is only meant to be used when the
        heuristics are to be applied, i.e., if a recalibration is required. The default value is None (meaning
        the recalibration is not required). In case this hyperparameter is set to a value other than None, but
        the recalibration is not required (recalib=None), a warning message will be raised.
    :param exact_train_prev: set to True (default) for using the true training prevalence as the initial observation;
        set to False for computing the training prevalence as an estimate of it, i.e., as the expected
        value of the posterior probabilities of the training instances.
    :param recalib: a string indicating the method of recalibration.
        Available choices include "nbvs" (No-Bias Vector Scaling), "bcts" (Bias-Corrected Temperature Scaling,
        default), "ts" (Temperature Scaling), and "vs" (Vector Scaling). Default is None (no recalibration).
    :param n_jobs: number of parallel workers. Only used for recalibrating the classifier if `val_split` is set to
        an integer `k` --the number of folds.
    """

    MAX_ITER = 1000
    EPSILON = 1e-4

    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None):
        self.classifier = qp._get_classifier(classifier)
        self.val_split = val_split
        self.exact_train_prev = exact_train_prev
        self.recalib = recalib
        self.n_jobs = n_jobs
        self.callback = callback
        self.log_trajectory = False
        self.C = None
        self.U = None
    

    @classmethod
    def EMQ_BCTS(cls, classifier: BaseEstimator, n_jobs=None):
        """
        Constructs an instance of EMQ using the best configuration found in the `Alexandari et al. paper
        <http://proceedings.mlr.press/v119/alexandari20a.html>`_, i.e., one that relies on Bias-Corrected Temperature
        Scaling (BCTS) as a recalibration function, and that uses an estimate of the training prevalence instead of
        the true training prevalence.

        :param classifier: a sklearn's Estimator that generates a classifier
        :param n_jobs: number of parallel workers.
        :return: An instance of EMQ with BCTS
        """
        return EMQ(classifier, val_split=5, exact_train_prev=False, recalib='bcts', n_jobs=n_jobs)

    def _check_init_parameters(self):
        if self.val_split is not None:
            if self.exact_train_prev and self.recalib is None:
                raise RuntimeWarning(f'The parameter {self.val_split=} was specified for EMQ, while the parameters '
                      f'{self.exact_train_prev=} and {self.recalib=}. This has no effect and causes an unnecessary '
                      f'overload.')
        else:
            if self.recalib is not None:
                print(f'[warning] The parameter {self.recalib=} requires the val_split be different from None. '
                      f'This parameter will be set to 5. To avoid this warning, set this value to a float value '
                      f'indicating the proportion of training data to be used as validation, or to an integer '
                      f'indicating the number of folds for kFCV.')
                self.val_split=5

    def classify(self, instances):
        """
        Provides the posterior probabilities for the given instances. If the classifier was required
        to be recalibrated, then these posteriors are recalibrated accordingly.

        :param instances: array-like of shape `(n_instances, n_dimensions,)`
        :return: np.ndarray of shape `(n_instances, n_classes,)` with posterior probabilities
        """
        posteriors = self.classifier.predict_proba(instances)
        if hasattr(self, 'calibration_function') and self.calibration_function is not None and self.calib_ok:
            try:
                cal_posteriors = self.calibration_function(posteriors)
                posteriors = cal_posteriors
            except:
                pass
        return posteriors

    def aggregation_fit(self, classif_predictions: LabelledCollection, data: LabelledCollection):
        """
        Trains the aggregation function of EMQ. This comes down to recalibrating the posterior probabilities
        ir requested.

        :param classif_predictions: a :class:`quapy.data.base.LabelledCollection` containing,
            as instances, the posterior probabilities issued by the classifier and, as labels, the true labels
        :param data: a :class:`quapy.data.base.LabelledCollection` consisting of the training data
        """
        if self.recalib is not None:
            P, y = classif_predictions.Xy
            if self.recalib == 'nbvs':
                calibrator = NoBiasVectorScaling()
            elif self.recalib == 'bcts':
                calibrator = TempScaling(bias_positions='all')
            elif self.recalib == 'ts':
                calibrator = TempScaling()
            elif self.recalib == 'vs':
                calibrator = VectorScaling()
            else:
                raise ValueError('invalid param argument for recalibration method; available ones are '
                                 '"nbvs", "bcts", "ts", and "vs".')

            if not np.issubdtype(y.dtype, np.number):
                y = np.searchsorted(data.classes_, y)
            self.calib_ok = True
            try:
                self.calibration_function = calibrator(P, np.eye(data.n_classes)[y], posterior_supplied=True)
            except:
                self.calib_ok = False

        if self.exact_train_prev:
            self.train_prevalence = data.prevalence()
        else:
            train_posteriors = classif_predictions.X
            if self.recalib is not None and self.calib_ok:
                train_posteriors = self.calibration_function(train_posteriors)
            self.train_prevalence = F.prevalence_from_probabilities(train_posteriors)

    

    def aggregate(self, classif_posteriors, epsilon=EPSILON):
        priors, posteriors, trajectory = self.EM(self.train_prevalence, classif_posteriors, epsilon)
        if self.log_trajectory:
            if self.callback is not None:
                self.callback(trajectory)
        return priors

    def predict_proba(self, instances, epsilon=EPSILON):
        """
        Returns the posterior probabilities updated by the EM algorithm.

        :param instances: np.ndarray of shape `(n_instances, n_dimensions)`
        :param epsilon: error tolerance
        :return: np.ndarray of shape `(n_instances, n_classes)`
        """
        classif_posteriors = self.classify(instances)
        priors, posteriors, _ = self.EM(self.train_prevalence, classif_posteriors, epsilon)
        return posteriors
    
    def _apply_C(self, posteriors):
        sig = inspect.signature(self.C)
        names = list(sig.parameters)
        if len(names) == 1:
            return self.C(posteriors)
        # one extra param; fetch its current value from self
        param_name = names[1]
        if not hasattr(self, param_name):
            raise ValueError("Expecting a parameter: "+param_name)
        return self.C(posteriors, getattr(self, param_name))
    
    def _apply_U(self, posteriors, qs, qs_new):
        "U function receives the posteriors, the computed prevalence and the previous computed prevalence"
        sig = inspect.signature(self.U)
        names = list(sig.parameters)
        if len(names) == 3:
            return self.U(posteriors,qs,qs_new)
        # one extra param; fetch its current value from self
        param_name = names[3]
        if not hasattr(self, param_name):
            raise ValueError("Expecting a parameter: "+param_name)
        return self.U(posteriors,qs,qs_new, getattr(self, param_name))


    def EM(self, tr_prev, posterior_probabilities, epsilon=EPSILON):
        """
        Computes the `Expectation Maximization` routine.

        :param tr_prev: array-like, the training prevalence
        :param posterior_probabilities: `np.ndarray` of shape `(n_instances, n_classes,)` with the
            posterior probabilities
        :param epsilon: float, the threshold different between two consecutive iterations
            to reach before stopping the loop
        :return: a tuple with the estimated prevalence values (shape `(n_classes,)`) and
            the corrected posterior probabilities (shape `(n_instances, n_classes,)`)
        """
        Px = posterior_probabilities
        

        Ptr = np.copy(tr_prev)

        if np.prod(Ptr) == 0:  # some entry is 0; we should smooth the values to avoid 0 division
            Ptr += epsilon
            Ptr /= Ptr.sum()

        qs = np.copy(Ptr)  # qs (the running estimate) is initialized as the training prevalence

        trajectory = []

        s, converged = 0, False
        qs_prev_ = None
        while not converged and s < self.MAX_ITER:
            # E-step:
            ps_unnormalized = (qs / Ptr) * Px
            ps = ps_unnormalized / ps_unnormalized.sum(axis=1, keepdims=True)

            if self.C is not None:
                ps = self._apply_C(ps)

            # M-step:
            qs_new = ps.mean(axis=0)
            if self.U is not None:
                qs = self._apply_U(ps,qs,qs_new)
            else:
                qs = qs_new
            trajectory.append(qs.copy())

            if qs_prev_ is not None and qp.error.mae(qs, qs_prev_) < epsilon and s > 10:
                converged = True

            qs_prev_ = qs
            s += 1

        if not converged:
            print('[warning] the method has reached the maximum number of iterations; it might have not converged')

        return qs, ps, trajectory
        


def C_PosteriorSmoothing(posterior_probabilities, epsilon_smoothing):
    Px = posterior_probabilities + epsilon_smoothing
    return Px / Px.sum(axis=1, keepdims=True)

def C_TempScaling(posterior_probabilities, tau):
    Px = np.power(posterior_probabilities, 1.0 / tau)
    Px /= Px.sum(axis=1, keepdims=True)
    return Px

def U_Damping(posterior_probabilities, qs, qs_new,damping):
    return damping * qs_new + (1 - damping) * qs

def U_DirichLetMAP(posterior_probabilities,qs,qs_new,alpha):
    N = posterior_probabilities.shape[0]
    K = posterior_probabilities.shape[1]
    return (posterior_probabilities.sum(axis=0) + alpha - 1) / (N + K * (alpha - 1))

def U_ConfidentSubset(posterior_probabilities,qs,qs_new,tau):
    if tau < 1.0:
        max_probs = posterior_probabilities.max(axis=1)
        threshold = np.quantile(max_probs, 1 - tau)
        subset_mask = max_probs >= threshold
        ps_subset = posterior_probabilities[subset_mask]
    else:
        ps_subset = posterior_probabilities
    return ps_subset.mean(axis=0)

def U_EntropyReg(posterior_probabilities,qs,qs_new,eta):
    lr = 0.1
    max_inner_iter = 100
    if eta == 0.0:
        qs = qs_new.copy()
    else:
        qs = optimize_with_entropy(qs_new, eta=eta, lr=lr, max_iter=max_inner_iter)

    return qs


def optimize_with_entropy(Q_hat, eta, lr, max_iter):
        """
        Projected gradient ascent to solve:
            max_Q sum_c Q_hat[c] * log(Q[c]) + eta * H(Q)
        subject to Q in simplex.
        """
        K = len(Q_hat)
        Q = np.full(K, 1.0 / K)

        for _ in range(max_iter):
            grad = Q_hat / (Q + 1e-12) + eta * (-1 - np.log(Q + 1e-12))
            Q += lr * grad
            Q = np.clip(Q, 1e-10, None)  # avoid zeros
            Q /= Q.sum()  # project back to simplex

        return Q


class EMQPosteriorSmoothing(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, epsilon_smoothing=1e-5):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.epsilon_smoothing=epsilon_smoothing
        self.C = C_PosteriorSmoothing


class EMQTempScaling(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, tau=1.0):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.tau=tau
        self.C = C_TempScaling

class EMQDamping(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, damping=0.5):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.damping=damping
        self.U = U_Damping

class EMQDirichletMAP(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, alpha=0.5):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.alpha=alpha
        self.U = U_DirichLetMAP


class EMQConfidentSubset(EMQ):
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True, recalib=None,
                 n_jobs=None, callback=None, tau=1.0):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        assert 0 < tau <= 1, "tau must be in (0, 1]"
        self.tau = tau
        self.U = U_ConfidentSubset


class EMQEntropyReg(EMQ):
    def __init__(self, classifier=None, val_split=None, exact_train_prev=True,
                 recalib=None, n_jobs=None, callback=None, eta=0.01):
        super().__init__(classifier, val_split, exact_train_prev, recalib, n_jobs, callback)
        self.eta = eta
        self.U = U_EntropyReg
        
class EMQTempScaling_DirichletMAP(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, tau=1.0,alpha=0.5):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.tau=tau
        self.alpha=alpha
        self.U = U_DirichLetMAP
        self.C = C_TempScaling

class EMQTempScaling_EntropyReg(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, tau=1.0,eta=0.01):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.tau=tau
        self.eta=eta
        self.U = U_EntropyReg
        self.C = C_TempScaling

class EMQTempScaling_Damping(EMQ):
    def __init__(self, classifier: BaseEstimator=None, val_split=None, exact_train_prev=True, recalib=None, n_jobs=None,callback=None, tau=1.0,damping=0.5):
        super().__init__(classifier=classifier, val_split=val_split,exact_train_prev=exact_train_prev,recalib=recalib,n_jobs=n_jobs,callback=callback)
        self.tau=tau
        self.damping=damping
        self.U = U_Damping
        self.C = C_TempScaling
    
    



